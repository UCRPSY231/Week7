{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multi Layer (Deep) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # for python 2 and 3 compatibility\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read csv\n",
    "all_charts = pd.read_csv('BillboardLyricData.txt', sep='\\t', encoding='utf-8')\n",
    "all_charts = all_charts.dropna()\n",
    "\n",
    "# countvecotrize data\n",
    "num_features = 500\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=0.0,max_features=num_features,stop_words='english')\n",
    "X = np.asarray(vectorizer.fit_transform(all_charts.lyrics).todense()).astype(np.float32)\n",
    "\n",
    "# y to ints\n",
    "labels = np.unique(all_charts.chart).tolist()\n",
    "num_labels = len(labels)\n",
    "class_mapping = {label:idx for idx,label in enumerate(labels)}\n",
    "y = all_charts.chart.map(class_mapping)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# scale\n",
    "std_scaler = StandardScaler()\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pure python implementation\n",
    "### http://cs231n.github.io/neural-networks-case-study/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.94805310665\n",
      "iteration 10: loss 1.43314164183\n",
      "iteration 20: loss 0.651841857816\n",
      "iteration 30: loss 0.35927634324\n",
      "iteration 40: loss 0.236928803375\n",
      "iteration 50: loss 0.187884594259\n",
      "iteration 60: loss 0.168370936537\n",
      "iteration 70: loss 0.158399948688\n",
      "iteration 80: loss 0.153905322709\n",
      "iteration 90: loss 0.149424519041\n",
      "iteration 100: loss 0.149823747186\n",
      "iteration 110: loss 0.143248722543\n",
      "iteration 120: loss 0.141453195741\n",
      "iteration 130: loss 0.141021974114\n",
      "iteration 140: loss 0.139124011079\n",
      "iteration 150: loss 0.137521723827\n",
      "iteration 160: loss 0.136973789946\n",
      "iteration 170: loss 0.135724038279\n",
      "iteration 180: loss 0.134972371029\n",
      "iteration 190: loss 0.134330789005\n",
      "iteration 200: loss 0.134503549468\n",
      "iteration 210: loss 0.134241714548\n",
      "iteration 220: loss 0.133845645424\n",
      "iteration 230: loss 0.133055274863\n",
      "iteration 240: loss 0.134202263066\n",
      "iteration 250: loss 0.134877750424\n",
      "iteration 260: loss 0.132074462909\n",
      "iteration 270: loss 0.131751540127\n",
      "iteration 280: loss 0.132559813672\n",
      "iteration 290: loss 0.130793495949\n",
      "iteration 300: loss 0.131981858998\n",
      "iteration 310: loss 0.130779198\n",
      "iteration 320: loss 0.130749013327\n",
      "iteration 330: loss 0.131187996321\n",
      "iteration 340: loss 0.12998584709\n",
      "iteration 350: loss 0.130683386102\n",
      "iteration 360: loss 0.129792613771\n",
      "iteration 370: loss 0.130151192497\n",
      "iteration 380: loss 0.129427710129\n",
      "iteration 390: loss 0.130088853796\n",
      "iteration 400: loss 0.129194319766\n",
      "iteration 410: loss 0.129131744609\n",
      "iteration 420: loss 0.128705158839\n",
      "iteration 430: loss 0.128619432011\n",
      "iteration 440: loss 0.128602371792\n",
      "iteration 450: loss 0.128379310209\n",
      "iteration 460: loss 0.128533974078\n",
      "iteration 470: loss 0.127991798124\n",
      "iteration 480: loss 0.12792727109\n",
      "iteration 490: loss 0.127893346595\n"
     ]
    }
   ],
   "source": [
    "K = num_labels # num classes/labels\n",
    "h = 100 # size of hidden layer\n",
    "D = num_features # num features\n",
    "\n",
    "# initialize weights randomly\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0 # learning rate\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X_train_std.shape[0]\n",
    "for i in xrange(500):\n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer = np.maximum(0, np.dot(X_train_std, W) + b) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y_train])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 10 == 0:\n",
    "        print('iteration {}: loss {}'.format(i, loss))\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y_train] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    # finally into W,b\n",
    "    dW = np.dot(X_train_std.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "\n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
